{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HMsvLg4q5bp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers accelerate\n",
        "!pip install pandas openpyxl\n",
        "!pip install transformers peft torch\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O6nX9xBFrF50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BMj6L47eryyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_AUTH_TOKEN\"] = \"Your_Access_Token\"\n"
      ],
      "metadata": {
        "id": "ZF6aR7ehrOGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "base_model_id = \"NousResearch/Llama-2-7b-hf\"  # Public and compatible\n",
        "adapter_model_id = 'Ashishkr/llama-2-medical-consultation'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load base model (standard float16 or 8-bit if needed)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    use_auth_token=os.environ[\"HF_AUTH_TOKEN\"]\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, adapter_model_id).to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    use_auth_token=os.environ[\"HF_AUTH_TOKEN\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "lk-E5HkIrdlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Upload your file\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Ayush/drug_interaction_dataset.xlsx\")\n",
        "\n",
        "# Create prompt format\n",
        "def create_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical expert. Analyze the following drug interaction and explain the likely outcome.\\n\"\n",
        "        f\"Drug A: {row['Drug_A']}\\n\"\n",
        "        f\"Drug B: {row['Drug_B']}\\n\"\n",
        "        f\"Mechanism: {row['Mechanism_of_Interaction']}\\n\"\n",
        "        f\"Outcome:\"\n",
        "    )\n",
        "\n",
        "# Apply to dataset\n",
        "df['prompt'] = df.apply(create_prompt, axis=1)\n",
        "\n",
        "# Show one example prompt\n",
        "print(df['prompt'].iloc[0])\n"
      ],
      "metadata": {
        "id": "AV2iblamwLnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, max_new_tokens=100, temperature=0.5):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt from the output to keep just the model's answer\n",
        "    cleaned = decoded.replace(prompt, \"\").strip()\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "tL4O13FQs36u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def medllama_interaction_response(drug_a, drug_b, mechanism):\n",
        "    # Use specific prompt structure\n",
        "    prompt = (\n",
        "    f\"You are a medical expert. Analyze the following drug interaction and explain the likely outcome.\\n\"\n",
        "    f\"Drug A: {drug_a}\\n\"\n",
        "    f\"Drug B: {drug_b}\\n\"\n",
        "    f\"Mechanism: {mechanism if mechanism else 'Unknown'}\\n\"\n",
        "    f\"Outcome:\"\n",
        ")\n",
        "\n",
        "\n",
        "    # Generate response using model\n",
        "    response = generate_response(prompt)\n",
        "    return response\n",
        "\n",
        "# Define the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=medllama_interaction_response,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Drug A\"),\n",
        "        gr.Textbox(label=\"Drug B\"),\n",
        "        gr.Textbox(label=\"Mechanism of Interaction (e.g., Inhibits clearance)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Predicted Interaction Outcome\"),\n",
        "    title=\"ðŸ§  MedLLaMA Drug Interaction Predictor\",\n",
        "    description=\"Enter two drugs and their mechanism of interaction. MedLLaMA will predict the likely outcome.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "3KhPmRjEryq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}